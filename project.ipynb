{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           13871\n",
       "candidate                    13775\n",
       "candidate_confidence         13871\n",
       "relevant_yn                  13871\n",
       "relevant_yn_confidence       13871\n",
       "sentiment                    13871\n",
       "sentiment_confidence         13871\n",
       "subject_matter               13545\n",
       "subject_matter_confidence    13871\n",
       "candidate_gold                  28\n",
       "name                         13871\n",
       "relevant_yn_gold                32\n",
       "retweet_count                13871\n",
       "sentiment_gold                  15\n",
       "subject_matter_gold             18\n",
       "text                         13871\n",
       "tweet_coord                     21\n",
       "tweet_created                13871\n",
       "tweet_id                     13871\n",
       "tweet_location                9959\n",
       "user_timezone                 9468\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab \n",
    "import nltk\n",
    "\n",
    "from sklearn import tree\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#read csv into dataframe\n",
    "GOPSentiDf = pd.read_csv('2016GOPPresDebSenti.csv')\n",
    "AirlineSentiDf = pd.read_csv('TweetsUSAirlineSenti.csv')\n",
    "\n",
    "#Begin feature removal\n",
    "GOPSentiDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                        14640\n",
       "airline_sentiment               14640\n",
       "airline_sentiment_confidence    14640\n",
       "negativereason                   9178\n",
       "negativereason_confidence       10522\n",
       "airline                         14640\n",
       "airline_sentiment_gold             40\n",
       "name                            14640\n",
       "negativereason_gold                32\n",
       "retweet_count                   14640\n",
       "text                            14640\n",
       "tweet_coord                      1019\n",
       "tweet_created                   14640\n",
       "tweet_location                   9907\n",
       "user_timezone                    9820\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AirlineSentiDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed unnecessary columns\n"
     ]
    }
   ],
   "source": [
    "#we want the sentiment analysis to be as general as possible, independent of user location, when they tweeted, \n",
    "#the tweet's subject matter, how often their tweets get retweeted, who is the user, \n",
    "\n",
    "GOPSentiDf.drop('user_timezone', 1, inplace = True)\n",
    "GOPSentiDf.drop('tweet_location', 1, inplace = True)\n",
    "GOPSentiDf.drop('tweet_id', 1, inplace = True)\n",
    "GOPSentiDf.drop('tweet_created', 1, inplace = True)\n",
    "GOPSentiDf.drop('tweet_coord', 1, inplace = True)\n",
    "#subject matter gold is the specific topic of tweeted text, such as Religion, Abortion, Immigration, FOX news, etc.\n",
    "GOPSentiDf.drop('subject_matter_gold', 1, inplace = True)\n",
    "#sentiment gold is repetitive of the sentiment column\n",
    "GOPSentiDf.drop('sentiment_gold', 1, inplace = True)\n",
    "GOPSentiDf.drop('retweet_count', 1, inplace = True)\n",
    "#relevant_yn_gold is repetitive of relevant\n",
    "GOPSentiDf.drop('relevant_yn_gold', 1, inplace = True)\n",
    "GOPSentiDf.drop('name', 1, inplace = True)\n",
    "#candidate_gold is repetitive of candidate\n",
    "GOPSentiDf.drop('candidate_gold', 1, inplace = True)\n",
    "GOPSentiDf.drop('subject_matter_confidence', 1, inplace = True)\n",
    "GOPSentiDf.drop('subject_matter', 1, inplace = True)\n",
    "GOPSentiDf.drop('relevant_yn_confidence', 1, inplace = True)\n",
    "GOPSentiDf.drop('relevant_yn', 1, inplace = True)\n",
    "GOPSentiDf.drop('candidate_confidence', 1, inplace = True)\n",
    "GOPSentiDf.drop('candidate', 1, inplace = True)\n",
    "\n",
    "#we want to do the same for the tweets for the US Airlines\n",
    "AirlineSentiDf.drop('user_timezone', 1, inplace = True)\n",
    "AirlineSentiDf.drop('tweet_location', 1, inplace = True)\n",
    "AirlineSentiDf.drop('tweet_created', 1, inplace = True)\n",
    "AirlineSentiDf.drop('tweet_coord', 1, inplace = True)\n",
    "AirlineSentiDf.drop('retweet_count', 1, inplace = True)\n",
    "AirlineSentiDf.drop('negativereason_gold', 1, inplace = True)\n",
    "AirlineSentiDf.drop('name', 1, inplace = True)\n",
    "AirlineSentiDf.drop('airline_sentiment_gold', 1, inplace = True)\n",
    "AirlineSentiDf.drop('airline', 1, inplace = True)\n",
    "AirlineSentiDf.drop('negativereason_confidence', 1, inplace = True)\n",
    "AirlineSentiDf.drop('negativereason', 1, inplace = True)\n",
    "\n",
    "print(\"removed unnecessary columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                      13871\n",
       "sentiment               13871\n",
       "sentiment_confidence    13871\n",
       "text                    13871\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOPSentiDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                        14640\n",
       "airline_sentiment               14640\n",
       "airline_sentiment_confidence    14640\n",
       "text                            14640\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AirlineSentiDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined the 2 dataframes\n"
     ]
    }
   ],
   "source": [
    "#combine GOPSentiDf and AirlineSentiDf\n",
    "GOPIdAr = GOPSentiDf.values[0:,][:,0]\n",
    "GOPSentiAr = GOPSentiDf.values[0:,][:,1]\n",
    "GOPSentiConAr = GOPSentiDf.values[0:,][:,2]\n",
    "GOPTxtAr= GOPSentiDf.values[0:,][:,3]\n",
    "\n",
    "AirIdAr = AirlineSentiDf.values[0:,][:,0]\n",
    "AirSentiAr = AirlineSentiDf.values[0:,][:,1]\n",
    "AirSentiConAr = AirlineSentiDf.values[0:,][:,2]\n",
    "AirTxtAr = AirlineSentiDf.values[0:,][:,3]\n",
    "\n",
    "IdAr = []\n",
    "SentiAr = []\n",
    "SentiConAr = []\n",
    "TxtAr = []\n",
    "\n",
    "for i in range(len(GOPIdAr)):\n",
    "    IdAr.append(GOPIdAr[i])\n",
    "    SentiAr.append(GOPSentiAr[i])\n",
    "    SentiConAr.append(GOPSentiConAr[i])\n",
    "    TxtAr.append(GOPTxtAr[i])\n",
    "\n",
    "for i in range(len(AirIdAr)):\n",
    "    IdAr.append(AirIdAr[i])\n",
    "    SentiAr.append(AirSentiAr[i])\n",
    "    SentiConAr.append(AirSentiConAr[i])\n",
    "    TxtAr.append(AirTxtAr[i])\n",
    "\n",
    "print(\"combined the 2 dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned the text and updated both text columns\n"
     ]
    }
   ],
   "source": [
    "#remove all stopwords, hashtags, web links, retweets (RT), direct @s, and symbols\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "shrtTxtAr = []\n",
    "index = 0\n",
    "for i in range(len(TxtAr)):\n",
    "    shrtTxtAr.append('')\n",
    "for text in TxtAr:\n",
    "    TxtAr[index] = TxtAr[index].replace('RT ', '')\n",
    "    while (TxtAr[index].find('#') != -1):\n",
    "        TxtAr[index] = TxtAr[index].replace('#', '')\n",
    "    while (TxtAr[index].find('@') != -1):\n",
    "        TxtAr[index] = TxtAr[index].replace('@', '')\n",
    "    while (TxtAr[index].find('http') != -1):\n",
    "        cnt = TxtAr[index].find('http')\n",
    "        TxtAr[index] = TxtAr[index].replace(TxtAr[index][cnt:], '')\n",
    "    text = TxtAr[index]\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        if (w.isalpha() and w not in stop_words):\n",
    "            #stem all the words for easier classification later\n",
    "            w = stemmer.stem(w)\n",
    "            shrtTxtAr[index] = shrtTxtAr[index] + ' ' + w\n",
    "    TxtAr[index] = shrtTxtAr[index]\n",
    "    index += 1\n",
    "print(\"cleaned the text and updated both text columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created positive, negative, and neutral arrays of words\n"
     ]
    }
   ],
   "source": [
    "#collect and categorize all words as whether they're positive, negative, or neutral according to how they were classified\n",
    "posWordsFreq = []\n",
    "negWordsFreq = []\n",
    "neuWordsFreq = []\n",
    "\n",
    "for i in range(len(SentiAr)):\n",
    "    words = word_tokenize(TxtAr[i])\n",
    "    if (SentiAr[i].lower() == 'positive'):\n",
    "        for w in range(len(words)):\n",
    "            posWordsFreq.append(words[w])\n",
    "    elif (SentiAr[i].lower() == 'negative'):\n",
    "        for w in range(len(words)):\n",
    "            negWordsFreq.append(words[w])\n",
    "    else:\n",
    "        for w in range(len(words)):\n",
    "            neuWordsFreq.append(words[w])\n",
    "\n",
    "posWordsFreq = nltk.FreqDist(posWordsFreq)\n",
    "negWordsFreq = nltk.FreqDist(negWordsFreq)\n",
    "neuWordsFreq = nltk.FreqDist(neuWordsFreq)\n",
    "\n",
    "posWordsKeysAr = list(posWordsFreq.keys())\n",
    "posWordsValuesAr = list(posWordsFreq.values())\n",
    "negWordsKeysAr = list(negWordsFreq.keys())\n",
    "negWordsValuesAr = list(negWordsFreq.values())\n",
    "neuWordsKeysAr = list(neuWordsFreq.keys())\n",
    "neuWordsValuesAr = list(neuWordsFreq.values())\n",
    "\n",
    "print(\"created positive, negative, and neutral arrays of words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counted all positive, negative, and neutral words per text line\n"
     ]
    }
   ],
   "source": [
    "#create features: number of negative and positive words and emoticons, number of emoticons\n",
    "NumPosAr = []\n",
    "NumNegAr = []\n",
    "NumNeuAr = []\n",
    "\n",
    "#determine if a word is more positive, negative, or neutral and store at one of three previously instantiated arrays\n",
    "for texts in TxtAr:\n",
    "    numPos = 0\n",
    "    numNeg = 0\n",
    "    numNeu = 0\n",
    "    words = word_tokenize(texts)\n",
    "    for w in words:\n",
    "        posVal = 0\n",
    "        negVal = 0\n",
    "        neuVal = 0\n",
    "        if w in posWordsKeysAr:\n",
    "            posVal = posWordsValuesAr[posWordsKeysAr.index(w)]\n",
    "        if w in negWordsKeysAr:\n",
    "            negVal = negWordsValuesAr[negWordsKeysAr.index(w)]\n",
    "        if w in neuWordsKeysAr:\n",
    "            neuVal = neuWordsValuesAr[neuWordsKeysAr.index(w)]\n",
    "        if posVal == max(posVal, negVal, neuVal):\n",
    "            numPos += 1\n",
    "        if negVal == max(posVal, negVal, neuVal):\n",
    "            numNeg += 1\n",
    "        if neuVal == max(posVal, negVal, neuVal):\n",
    "            numNeu += 1\n",
    "    NumPosAr.append(numPos)\n",
    "    NumNegAr.append(numNeg)\n",
    "    NumNeuAr.append(numNeu)\n",
    "\n",
    "print(\"counted all positive, negative, and neutral words per text line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all the arrays into a dataframe\n",
    "df = pd.DataFrame({'ID':IdAr, 'Text':TxtAr, '#Positive Words':NumPosAr, '#Negative Words':NumNegAr, '#Neutral Words':NumNeuAr,\n",
    "                  'Sentiment Confidence':SentiConAr, 'Sentiment':SentiAr})\n",
    "df = df[['ID', 'Text', '#Positive Words', '#Negative Words', '#Neutral Words', 'Sentiment Confidence', 'Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                      28511\n",
       "Text                    28511\n",
       "#Positive Words         28511\n",
       "#Negative Words         28511\n",
       "#Neutral Words          28511\n",
       "Sentiment Confidence    28511\n",
       "Sentiment               28511\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [2 3] TEST: [0 1]\n",
      "X_train:[[5 6]\n",
      " [7 8]], X_test:[[1 2]\n",
      " [3 4]], y_train[3 4], y_test:[1 2]\n",
      "TRAIN: [0 1] TEST: [2 3]\n",
      "X_train:[[1 2]\n",
      " [3 4]], X_test:[[5 6]\n",
      " [7 8]], y_train[1 2], y_test:[3 4]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    print(\"X_train:\" + str(X[train_index]) + \", X_test:\" + str(X[test_index]) + \", y_train\" + str(y[train_index]) + \", y_test:\" + str(y[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classify using Naive Bayes and SVM with k-fold\n",
    "clfNB = MultinomialNB()\n",
    "clfSVM = SVC()\n",
    "\n",
    "X = df.values[0:,[1, 2, 3, 4, 5]]\n",
    "Y = df.values[0:,][:,6]\n",
    "\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 11)\n",
    "kf = kFold(n_splits = 5, shuffle = True, random_state = 11)\n",
    "for train_index, text_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    clfNB.fit(X_train, Y_train)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nancyleegrahn how everyon feel climat chang question last night exact gopdeb\n",
      "0\n",
      "11\n",
      "1\n",
      "0.6578\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "print(TxtAr[0])\n",
    "print(NumPosAr[0])\n",
    "print(NumNegAr[0])\n",
    "print(NumNeuAr[0])\n",
    "print(SentiConAr[0])\n",
    "print(SentiAr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ' nancyleegrahn how everyon feel climat chang question last night exact gopdeb'\n",
      " 0 11 1 0.6578]\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need to define a function that breaks down a text into several features: text, number of positive, negative, and neutral things,\n",
    "#sentiment confidence. Then classifier classifies it to negative, positive, or neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of emotions, not emojis\n",
    "#create 2d array with 50 columns and 3 rows, where the row determines the number of character per emoji\n",
    "#i.e., :) contains 2 characters so it belongs it row 0, while :-D goes to row 1, and so on\n",
    "PosEmoAr = [[':)', ':]', ':}', '=)', '=]', '=}', ':B', '=B', '<3', '^^', ':*', '=*', ';)', ';]', \n",
    "             ';}', '=P', '=p', ':P', ':p', ':b', '=b'], \n",
    "            [':o)', ':o]', ':o}', ':-]', ':-)', ':-}', '=^]', '=^)', '=^}', ':-D', ':-B', \n",
    "             ':^D', ':^B', '=^B', '=^D', ':\\')', ':\\']', '=\\'}', '^.^', '^-^', '^_^', ':-*', \n",
    "             ':-p', ':-P', ':-b', ':^p', ':^P', ':^b', '\\\\o\\\\', '/o/', '=^p', '=^P', '=^b', '\\\\o/']]\n",
    "NegEmoAr = [['D:', 'D=', ':(', ':[', ':{', '=(', '=[', '={', '=\\\\', ':\\\\', '=/', ':/', '=$', 'Oo'], \n",
    "            ['D-:', 'D^:', 'D^=', ':o(', ':o[', ':^(', ':^[', ':^{', '=^(', '=^{', '>=(', '>=[', '>={', \n",
    "             ':-[', ':-(', '=^[', '>=[', ':\\'(', ':\\'[', ':\\'{', '=\\'{', '=\\'(', '=\\'[', 'o.O', 'O_o', ':o{'],\n",
    "            ['>:-{', '>:-[', '>:-(', '>=^[', '>=^(', '>:-{', '>=^{'],\n",
    "            ['>:-=(', ':$:-{']]\n",
    "NeuEmoAr = [[':|', '=|', '><', ':o', ':O', '=0', ':@', '=@', ':x', '=X', ':#', '=#'], \n",
    "            [':-|', '>.<', '>_<', ':^o', ':^@', '-.-', '-_-', ':-x', ':-X', ':-@', ':-#', ':^x', ':^#'], \n",
    "            ['-.-\\'', '-_-\\'']]\n",
    "\n",
    "print(\"created emoticons comparator\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
