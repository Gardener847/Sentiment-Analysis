{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           13871\n",
       "candidate                    13775\n",
       "candidate_confidence         13871\n",
       "relevant_yn                  13871\n",
       "relevant_yn_confidence       13871\n",
       "sentiment                    13871\n",
       "sentiment_confidence         13871\n",
       "subject_matter               13545\n",
       "subject_matter_confidence    13871\n",
       "candidate_gold                  28\n",
       "name                         13871\n",
       "relevant_yn_gold                32\n",
       "retweet_count                13871\n",
       "sentiment_gold                  15\n",
       "subject_matter_gold             18\n",
       "text                         13871\n",
       "tweet_coord                     21\n",
       "tweet_created                13871\n",
       "tweet_id                     13871\n",
       "tweet_location                9959\n",
       "user_timezone                 9468\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split #holdout method\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#read csv into dataframe\n",
    "GOPSentiDf = pd.read_csv('2016GOPPresDebSenti.csv')\n",
    "AirlineSentiDf = pd.read_csv('TweetsUSAirlineSenti.csv')\n",
    "\n",
    "#Begin feature removal\n",
    "GOPSentiDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                        14640\n",
       "airline_sentiment               14640\n",
       "airline_sentiment_confidence    14640\n",
       "negativereason                   9178\n",
       "negativereason_confidence       10522\n",
       "airline                         14640\n",
       "airline_sentiment_gold             40\n",
       "name                            14640\n",
       "negativereason_gold                32\n",
       "retweet_count                   14640\n",
       "text                            14640\n",
       "tweet_coord                      1019\n",
       "tweet_created                   14640\n",
       "tweet_location                   9907\n",
       "user_timezone                    9820\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AirlineSentiDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed unnecessary columns\n"
     ]
    }
   ],
   "source": [
    "#we want the sentiment analysis to be as general as possible, independent of user location, when they tweeted, \n",
    "#the tweet's subject matter, how often their tweets get retweeted, who is the user, \n",
    "\n",
    "GOPSentiDf.drop('user_timezone', 1, inplace = True)\n",
    "GOPSentiDf.drop('tweet_location', 1, inplace = True)\n",
    "GOPSentiDf.drop('tweet_id', 1, inplace = True)\n",
    "GOPSentiDf.drop('tweet_created', 1, inplace = True)\n",
    "GOPSentiDf.drop('tweet_coord', 1, inplace = True)\n",
    "#subject matter gold is the specific topic of tweeted text, such as Religion, Abortion, Immigration, FOX news, etc.\n",
    "GOPSentiDf.drop('subject_matter_gold', 1, inplace = True)\n",
    "#sentiment gold is repetitive of the sentiment column\n",
    "GOPSentiDf.drop('sentiment_gold', 1, inplace = True)\n",
    "GOPSentiDf.drop('retweet_count', 1, inplace = True)\n",
    "#relevant_yn_gold is repetitive of relevant\n",
    "GOPSentiDf.drop('relevant_yn_gold', 1, inplace = True)\n",
    "GOPSentiDf.drop('name', 1, inplace = True)\n",
    "#candidate_gold is repetitive of candidate\n",
    "GOPSentiDf.drop('candidate_gold', 1, inplace = True)\n",
    "GOPSentiDf.drop('subject_matter_confidence', 1, inplace = True)\n",
    "GOPSentiDf.drop('subject_matter', 1, inplace = True)\n",
    "GOPSentiDf.drop('relevant_yn_confidence', 1, inplace = True)\n",
    "GOPSentiDf.drop('relevant_yn', 1, inplace = True)\n",
    "GOPSentiDf.drop('candidate_confidence', 1, inplace = True)\n",
    "GOPSentiDf.drop('candidate', 1, inplace = True)\n",
    "\n",
    "#we want to do the same for the tweets for the US Airlines\n",
    "AirlineSentiDf.drop('user_timezone', 1, inplace = True)\n",
    "AirlineSentiDf.drop('tweet_location', 1, inplace = True)\n",
    "AirlineSentiDf.drop('tweet_created', 1, inplace = True)\n",
    "AirlineSentiDf.drop('tweet_coord', 1, inplace = True)\n",
    "AirlineSentiDf.drop('retweet_count', 1, inplace = True)\n",
    "AirlineSentiDf.drop('negativereason_gold', 1, inplace = True)\n",
    "AirlineSentiDf.drop('name', 1, inplace = True)\n",
    "AirlineSentiDf.drop('airline_sentiment_gold', 1, inplace = True)\n",
    "AirlineSentiDf.drop('airline', 1, inplace = True)\n",
    "AirlineSentiDf.drop('negativereason_confidence', 1, inplace = True)\n",
    "AirlineSentiDf.drop('negativereason', 1, inplace = True)\n",
    "\n",
    "print(\"removed unnecessary columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                      13871\n",
       "sentiment               13871\n",
       "sentiment_confidence    13871\n",
       "text                    13871\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GOPSentiDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                        14640\n",
       "airline_sentiment               14640\n",
       "airline_sentiment_confidence    14640\n",
       "text                            14640\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AirlineSentiDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined the 2 dataframes\n"
     ]
    }
   ],
   "source": [
    "#combine GOPSentiDf and AirlineSentiDf\n",
    "GOPIdAr = GOPSentiDf.values[0:,][:,0]\n",
    "GOPSentiAr = GOPSentiDf.values[0:,][:,1]\n",
    "GOPSentiConAr = GOPSentiDf.values[0:,][:,2]\n",
    "GOPTxtAr= GOPSentiDf.values[0:,][:,3]\n",
    "\n",
    "AirIdAr = AirlineSentiDf.values[0:,][:,0]\n",
    "AirSentiAr = AirlineSentiDf.values[0:,][:,1]\n",
    "AirSentiConAr = AirlineSentiDf.values[0:,][:,2]\n",
    "AirTxtAr = AirlineSentiDf.values[0:,][:,3]\n",
    "\n",
    "IdAr = []\n",
    "SentiAr = []\n",
    "SentiConAr = []\n",
    "TxtAr = []\n",
    "\n",
    "for i in range(len(GOPIdAr)):\n",
    "    IdAr.append(GOPIdAr[i])\n",
    "    SentiAr.append(GOPSentiAr[i])\n",
    "    SentiConAr.append(GOPSentiConAr[i])\n",
    "    TxtAr.append(GOPTxtAr[i])\n",
    "\n",
    "#for i in range(len(AirIdAr)):\n",
    "#    IdAr.append(AirIdAr[i])\n",
    "#    SentiAr.append(AirSentiAr[i])\n",
    "#    SentiConAr.append(AirSentiConAr[i])\n",
    "#    TxtAr.append(AirTxtAr[i])\n",
    "\n",
    "print(\"combined the 2 dataframes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned the text and updated both text columns\n"
     ]
    }
   ],
   "source": [
    "#remove all stopwords, hashtags, web links, retweets (RT), direct @s, and symbols\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "shrtTxtAr = []\n",
    "index = 0\n",
    "for i in range(len(TxtAr)):\n",
    "    shrtTxtAr.append('')\n",
    "for text in TxtAr:\n",
    "    TxtAr[index] = TxtAr[index].replace('RT ', '')\n",
    "    while (TxtAr[index].find('#') != -1):\n",
    "        TxtAr[index] = TxtAr[index].replace('#', '')\n",
    "    while (TxtAr[index].find('@') != -1):\n",
    "        TxtAr[index] = TxtAr[index].replace('@', '')\n",
    "    while (TxtAr[index].find('http') != -1):\n",
    "        cnt = TxtAr[index].find('http')\n",
    "        TxtAr[index] = TxtAr[index].replace(TxtAr[index][cnt:], '')\n",
    "    text = TxtAr[index]\n",
    "    words = word_tokenize(text)\n",
    "    for w in words:\n",
    "        if (w.isalpha() and w not in stop_words):\n",
    "            #stem all the words for easier classification later\n",
    "            w = stemmer.stem(w)\n",
    "            shrtTxtAr[index] = shrtTxtAr[index] + ' ' + w\n",
    "    TxtAr[index] = shrtTxtAr[index]\n",
    "    index += 1\n",
    "print(\"cleaned the text and updated both text columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created positive, negative, and neutral arrays of words\n"
     ]
    }
   ],
   "source": [
    "#collect and categorize all words as whether they're positive, negative, or neutral according to how they were classified\n",
    "posWordsFreq = []\n",
    "negWordsFreq = []\n",
    "neuWordsFreq = []\n",
    "\n",
    "for i in range(len(SentiAr)):\n",
    "    words = word_tokenize(TxtAr[i])\n",
    "    if (SentiAr[i].lower() == 'positive'):\n",
    "        for w in range(len(words)):\n",
    "            posWordsFreq.append(words[w])\n",
    "    elif (SentiAr[i].lower() == 'negative'):\n",
    "        for w in range(len(words)):\n",
    "            negWordsFreq.append(words[w])\n",
    "    else:\n",
    "        for w in range(len(words)):\n",
    "            neuWordsFreq.append(words[w])\n",
    "\n",
    "posWordsFreq = nltk.FreqDist(posWordsFreq)\n",
    "negWordsFreq = nltk.FreqDist(negWordsFreq)\n",
    "neuWordsFreq = nltk.FreqDist(neuWordsFreq)\n",
    "\n",
    "posWordsKeysAr = list(posWordsFreq.keys())\n",
    "posWordsValuesAr = list(posWordsFreq.values())\n",
    "negWordsKeysAr = list(negWordsFreq.keys())\n",
    "negWordsValuesAr = list(negWordsFreq.values())\n",
    "neuWordsKeysAr = list(neuWordsFreq.keys())\n",
    "neuWordsValuesAr = list(neuWordsFreq.values())\n",
    "\n",
    "print(\"created positive, negative, and neutral arrays of words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counted all positive, negative, and neutral words per text line\n"
     ]
    }
   ],
   "source": [
    "#create features: number of negative and positive words and emoticons, number of emoticons\n",
    "NumPosAr = []\n",
    "NumNegAr = []\n",
    "NumNeuAr = []\n",
    "\n",
    "#determine if a word is more positive, negative, or neutral and store at one of three previously instantiated arrays\n",
    "for texts in TxtAr:\n",
    "    numPos = 0\n",
    "    numNeg = 0\n",
    "    numNeu = 0\n",
    "    words = word_tokenize(texts)\n",
    "    for w in words:\n",
    "        posVal = 0\n",
    "        negVal = 0\n",
    "        neuVal = 0\n",
    "        if w in posWordsKeysAr:\n",
    "            posVal = posWordsValuesAr[posWordsKeysAr.index(w)]\n",
    "        if w in negWordsKeysAr:\n",
    "            negVal = negWordsValuesAr[negWordsKeysAr.index(w)]\n",
    "        if w in neuWordsKeysAr:\n",
    "            neuVal = neuWordsValuesAr[neuWordsKeysAr.index(w)]\n",
    "        if posVal == max(posVal, negVal, neuVal):\n",
    "            numPos += 1\n",
    "        if negVal == max(posVal, negVal, neuVal):\n",
    "            numNeg += 1\n",
    "        if neuVal == max(posVal, negVal, neuVal):\n",
    "            numNeu += 1\n",
    "    NumPosAr.append(numPos)\n",
    "    NumNegAr.append(numNeg)\n",
    "    NumNeuAr.append(numNeu)\n",
    "\n",
    "print(\"counted all positive, negative, and neutral words per text line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recombined to new dataframe\n"
     ]
    }
   ],
   "source": [
    "#combine all the arrays into a dataframe\n",
    "df = pd.DataFrame({'ID':IdAr, 'Text':TxtAr, '#Positive Words':NumPosAr, '#Negative Words':NumNegAr, '#Neutral Words':NumNeuAr,\n",
    "                  'Sentiment Confidence':SentiConAr, 'Sentiment':SentiAr})\n",
    "df = df[['ID', 'Text', '#Positive Words', '#Negative Words', '#Neutral Words', 'Sentiment Confidence', 'Sentiment']]\n",
    "\n",
    "print(\"Recombined to new dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                      13871\n",
       "Text                    13871\n",
       "#Positive Words         13871\n",
       "#Negative Words         13871\n",
       "#Neutral Words          13871\n",
       "Sentiment Confidence    13871\n",
       "Sentiment               13871\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Multinomial Naive Bayes, Linear SVM, Logarithmic Regression, and Ensemble Voting Classifier models\n"
     ]
    }
   ],
   "source": [
    "#classify using Naive Bayes, SVM, and maximum entropy with k-fold\n",
    "X = df.values[0:,[2, 3, 4, 5]]\n",
    "Y = df.values[0:,][:,6]\n",
    "\n",
    "seed = 11\n",
    "\n",
    "kfold = model_selection.KFold(n_splits = 10, random_state = seed)\n",
    "\n",
    "clfNB = MultinomialNB(alpha = 1) #alpha for Laplacian correction\n",
    "clfSVM = LinearSVC(random_state = seed)\n",
    "clfLogReg = LogisticRegression(random_state = seed)\n",
    "eclf = VotingClassifier(estimators=[('MultiNm', clfNB), ('LinSVC', clfSVM), ('lr', clfLogReg)], voting = 'hard') #hard voting for majority voting\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = seed)\n",
    "clfNB.fit(X_train, Y_train)\n",
    "clfSVM.fit(X_train, Y_train)\n",
    "clfLogReg.fit(X_train, Y_train)\n",
    "eclf.fit(X_train, Y_train)\n",
    "\n",
    "predNB = clfNB.predict(X_test)\n",
    "predSVM = clfSVM.predict(X_test)\n",
    "predLogReg = clfLogReg.predict(X_test)\n",
    "predEns = eclf.predict(X_test)\n",
    "\n",
    "print(\"Trained Multinomial Naive Bayes, Linear SVM, Logarithmic Regression, and Ensemble Voting Classifier models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing classifier accuracies\n",
      "Multinomial Naive Bayes: 0.770184544406\n",
      "Linear SVM: 0.773644752018\n",
      "Logarithmic Regression: 0.775663206459\n",
      "Ensemble: 0.77508650519\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing classifier accuracies\")\n",
    "print(\"Multinomial Naive Bayes: \" + str(accuracy_score(Y_test, predNB)))\n",
    "print(\"Linear SVM: \" + str(accuracy_score(Y_test, predSVM)))\n",
    "print(\"Logarithmic Regression: \" + str(accuracy_score(Y_test, predLogReg)))\n",
    "print(\"Ensemble: \" + str(accuracy_score(Y_test, predEns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing classifier precisions\n",
      "Multinomial Naive Bayes: 0.763552176925\n",
      "Linear SVM: 0.772890291507\n",
      "Logarithmic Regression: 0.771619049718\n",
      "Ensemble: 0.772503538047\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing classifier precisions\")\n",
    "print(\"Multinomial Naive Bayes: \" + str(precision_score(Y_test, predNB, average='weighted')))\n",
    "print(\"Linear SVM: \" + str(precision_score(Y_test, predSVM, average='weighted')))\n",
    "print(\"Logarithmic Regression: \" + str(precision_score(Y_test, predLogReg, average='weighted')))\n",
    "print(\"Ensemble: \" + str(precision_score(Y_test, predEns, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing classifier recalls\n",
      "Multinomial Naive Bayes: 0.770184544406\n",
      "Linear SVM: 0.773644752018\n",
      "Logarithmic Regression: 0.775663206459\n",
      "Ensemble: 0.77508650519\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing classifier recalls\")\n",
    "print(\"Multinomial Naive Bayes: \" + str(recall_score(Y_test, predNB, average='weighted')))\n",
    "print(\"Linear SVM: \" + str(recall_score(Y_test, predSVM, average='weighted')))\n",
    "print(\"Logarithmic Regression: \" + str(recall_score(Y_test, predLogReg, average='weighted')))\n",
    "print(\"Ensemble: \" + str(recall_score(Y_test, predEns, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created emoticons comparator\n"
     ]
    }
   ],
   "source": [
    "#list of emotions, not emojis\n",
    "#create 2d array with 50 columns and 3 rows, where the row determines the number of character per emoji\n",
    "#i.e., :) contains 2 characters so it belongs it row 0, while :-D goes to row 1, and so on\n",
    "PosEmoAr = [[':)', ':]', ':}', '=)', '=]', '=}', ':B', '=B', '<3', '^^', ':*', '=*', ';)', ';]', \n",
    "             ';}', '=P', '=p', ':P', ':p', ':b', '=b'], \n",
    "            [':o)', ':o]', ':o}', ':-]', ':-)', ':-}', '=^]', '=^)', '=^}', ':-D', ':-B', \n",
    "             ':^D', ':^B', '=^B', '=^D', ':\\')', ':\\']', '=\\'}', '^.^', '^-^', '^_^', ':-*', \n",
    "             ':-p', ':-P', ':-b', ':^p', ':^P', ':^b', '\\\\o\\\\', '/o/', '=^p', '=^P', '=^b', '\\\\o/']]\n",
    "NegEmoAr = [['D:', 'D=', ':(', ':[', ':{', '=(', '=[', '={', '=\\\\', ':\\\\', '=/', ':/', '=$', 'Oo'], \n",
    "            ['D-:', 'D^:', 'D^=', ':o(', ':o[', ':^(', ':^[', ':^{', '=^(', '=^{', '>=(', '>=[', '>={', \n",
    "             ':-[', ':-(', '=^[', '>=[', ':\\'(', ':\\'[', ':\\'{', '=\\'{', '=\\'(', '=\\'[', 'o.O', 'O_o', ':o{'],\n",
    "            ['>:-{', '>:-[', '>:-(', '>=^[', '>=^(', '>:-{', '>=^{'],\n",
    "            ['>:-=(', ':$:-{']]\n",
    "NeuEmoAr = [[':|', '=|', '><', ':o', ':O', '=0', ':@', '=@', ':x', '=X', ':#', '=#'], \n",
    "            [':-|', '>.<', '>_<', ':^o', ':^@', '-.-', '-_-', ':-x', ':-X', ':-@', ':-#', ':^x', ':^#'], \n",
    "            ['-.-\\'', '-_-\\'']]\n",
    "\n",
    "print(\"created emoticons comparator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Web Application\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin Web Application\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#need to define a function that breaks down a text into several features: text, number of positive, negative, and neutral \n",
    "#things, sentiment confidence. Then classifier classifies it to negative, positive, or neutral\n",
    "from flask import Flask, request, render_template\n",
    "\n",
    "#for checking if text includes emoticons\n",
    "def incrCnt(cAr, ch):\n",
    "    if (ch in PosEmoAr[0:][0] or ch in PosEmoAr[0:][1]):\n",
    "        cAr[0] += 1\n",
    "    elif (ch in NegEmoAr[0:][0] or ch in NegEmoAr[0:][1] or ch in NegEmoAr[0:][2] or ch in NegEmoAr[0:][3]):\n",
    "        cAr[1] += 1\n",
    "    elif (ch in NeuEmoAr[0:][0] or ch in NeuEmoAr[0:][1] or ch in NeuEmoAr[0:][2]):\n",
    "        cAr[2] += 1\n",
    "    return cAr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defined the function\n"
     ]
    }
   ],
   "source": [
    "def sentiment(emo):\n",
    "    #for keeping track of the number of positive, negative, and neutral emoticons and words\n",
    "    cntAr = []\n",
    "    for i in range(3):\n",
    "        cntAr.append(0)\n",
    "    \n",
    "    #emoticons block\n",
    "    #case 1: emoticon contains regular symbols (done)\n",
    "    #case 2: emoticon contains letters (done)\n",
    "    #case 3: emoticon contains both symbols and letters\n",
    "    #    a) letter can be in the beginning of the emoticon (done)\n",
    "    #    b) letter can be in the middle of the emoticon (done)\n",
    "    #    c) letter can be in the end of the emoticon (done)\n",
    "    index = 0\n",
    "    while index != len(emo):\n",
    "        if index >= len(emo) - 1:\n",
    "            break\n",
    "        #emoticons containing letters in the beginning\n",
    "        if (emo[index].isalpha() == True and index < (len(emo) - 1) and emo[index + 1].isalpha() == False and \n",
    "            emo[index + 1] != ' ' and emo[index] != ' '):\n",
    "            if (emo.find(' ', index) == -1):\n",
    "                ch = emo[index : len(emo)]\n",
    "                cntAr = incrCnt(cntAr, ch).copy()\n",
    "                emo = emo.replace(emo[emo.find(ch) : len(emo)], '')\n",
    "                index =  len(emo)\n",
    "            else:\n",
    "                ch = emo[index : emo.find(' ', index)]\n",
    "                cntAr = incrCnt(cntAr, ch).copy()\n",
    "                emo = emo.replace(emo[emo.find(ch) : emo.find(' ', index)] + ' ', '')\n",
    "                index =  emo.find(' ', index) + 1\n",
    "            continue\n",
    "        #emoticons containing pure symbols, and emoticon containing both symbols and letters\n",
    "        #but the letters are only located in between or in the end of symbols\n",
    "        if (emo[index].isalpha() == False and emo[index] != ' '):\n",
    "            if (emo.find(' ', index) == -1):\n",
    "                ch = emo[index : len(emo)]\n",
    "                cntAr = incrCnt(cntAr, ch).copy()\n",
    "                emo = emo.replace(emo[emo.find(ch) : len(emo)], '')\n",
    "                index =  len(emo)\n",
    "            else:\n",
    "                ch = emo[index : emo.find(' ', index)]\n",
    "                cntAr = incrCnt(cntAr, ch).copy()\n",
    "                emo = emo.replace(emo[emo.find(ch) : emo.find(' ', index)] + ' ', '')\n",
    "                index =  emo.find(' ', index) + 1\n",
    "            continue\n",
    "        index += 1 \n",
    "\n",
    "    #block for counting number of positive, negative, and neutral words\n",
    "    words = word_tokenize(emo)\n",
    "    for w in words:\n",
    "        posVal = 0\n",
    "        negVal = 0\n",
    "        neuVal = 0\n",
    "        w = stemmer.stem(w)\n",
    "        if w in posWordsKeysAr:\n",
    "            posVal = posWordsValuesAr[posWordsKeysAr.index(w)]\n",
    "        elif w in negWordsKeysAr:\n",
    "            negVal = negWordsValuesAr[negWordsKeysAr.index(w)]\n",
    "        elif w in neuWordsKeysAr:\n",
    "            neuVal = neuWordsValuesAr[neuWordsKeysAr.index(w)]\n",
    "        #if word is not in any of the positive, negative, and neutral dictionaries, label word as neutral\n",
    "        else:\n",
    "            neuVal = 1\n",
    "        if posVal == max(posVal, negVal, neuVal):\n",
    "            cntAr[0] += 1\n",
    "        if negVal == max(posVal, negVal, neuVal):\n",
    "            cntAr[1] += 1\n",
    "        if neuVal == max(posVal, negVal, neuVal):\n",
    "            cntAr[2] += 1\n",
    "        \n",
    "    #for defining Sentiment Confidence feature\n",
    "    sentiCon = max(cntAr[0], cntAr[1], cntAr[2])/ (cntAr[0] + cntAr[1] + cntAr[2])\n",
    "    \n",
    "    #compile all analyzed data and predict\n",
    "    xText = [[cntAr[0], cntAr[1], cntAr[2], sentiCon], [cntAr[0], cntAr[1], cntAr[2], sentiCon]]\n",
    "    sentiClas = ''\n",
    "    if ((cntAr[0] == cntAr[1] == cntAr[2]) or ((cntAr[1] == cntAr[2]) and cntAr[0] == 0) or\n",
    "       ((cntAr[0] == cntAr[2]) and cntAr[1] == 0) or ((cntAr[0] == cntAr[1]) and cntAr[2] == 0)):\n",
    "        sentiClas = 'neutral'\n",
    "    elif (cntAr[1] == cntAr[2]) and cntAr[0] != 0:\n",
    "        sentiClas = 'positive'\n",
    "    elif (cntAr[0] == cntAr[2]) and cntAr[1] != 0:\n",
    "        sentiClas = 'negative'\n",
    "    elif (cntAr[0] == cntAr[1]) and cntAr[2] != 0:\n",
    "        sentiClas = 'neutral'\n",
    "    else:\n",
    "        ensPred = eclf.predict(xText)\n",
    "        sentiClas = ensPred[0]\n",
    "    \n",
    "    print(str(cntAr[0]) + ' ' + str(cntAr[1]) + ' ' + str(cntAr[2]) + ' ' + str(sentiCon))\n",
    "    \n",
    "    return sentiClas\n",
    "\n",
    "print(\"defined the function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "@app.route('/')\n",
    "def my_form():\n",
    "    return render_template('boi.html')\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "\n",
    "def my_form_post():\n",
    "    emo = request.form['text']\n",
    "    processed_text = sentiment(emo)\n",
    "    return processed_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
